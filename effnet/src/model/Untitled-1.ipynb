{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "from src.make_data import effnet_data\n",
    "import utils\n",
    "from src.model import custom_metric\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "# SET CONFIG Effnet\n",
    "\n",
    "cfg = utils.load_yaml(Path(\"../../config/config.yaml\"))\n",
    "#DATA PATH\n",
    "RSNA_2022_PATH = cfg[\"data\"][\"RSNA_2022_PATH\"]\n",
    "TRAIN_IMAGES_PATH = f'{RSNA_2022_PATH}/train_images'\n",
    "TEST_IMAGES_PATH = f'{RSNA_2022_PATH}/test_images'\n",
    "EFFNET_CHECKPOINTS_PATH = cfg[\"data\"][\"EFFNET_CHECKPOINTS_PATH\"]\n",
    "METADATA_PATH = cfg[\"data\"][\"METADATA_PATH\"]\n",
    "\n",
    "#PARAMETER OF EFFNET\n",
    "EFFNET_MAX_TRAIN_BATCHES = int(cfg[\"model\"][\"EFFNET_MAX_TRAIN_BATCHES\"])\n",
    "EFFNET_MAX_EVAL_BATCHES = int(cfg[\"model\"][\"EFFNET_MAX_EVAL_BATCHES\"])\n",
    "ONE_CYCLE_MAX_LR = float(cfg[\"model\"][\"ONE_CYCLE_MAX_LR\"])\n",
    "ONE_CYCLE_PCT_START = float(cfg[\"model\"][\"ONE_CYCLE_PCT_START\"])\n",
    "SAVE_CHECKPOINT_EVERY_STEP = int(cfg[\"model\"][\"SAVE_CHECKPOINT_EVERY_STEP\"])\n",
    "FRAC_LOSS_WEIGHT = float(cfg[\"model\"][\"FRAC_LOSS_WEIGHT\"])\n",
    "PREDICT_MAX_BATCHES = float(cfg[\"model\"][\"PREDICT_MAX_BATCHES\"])\n",
    "PREDICT_MAX_BATCHES = 5\n",
    "N_FOLDS = int(cfg[\"model\"][\"N_FOLDS\"])\n",
    "ONE_CYCLE_EPOCH = int(cfg[\"model\"][\"ONE_CYCLE_EPOCH\"])\n",
    "SEED = int(cfg[\"model\"][\"SEED\"])\n",
    "WEIGHTS = tv.models.efficientnet.EfficientNet_V2_S_Weights.DEFAULT\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if DEVICE == 'cuda':\n",
    "    BATCH_SIZE = cfg[\"model\"][\"BATCH_SIZE\"]\n",
    "else:\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "#Read csv data for slicing\n",
    "df_train = pd.read_csv(f'{RSNA_2022_PATH}/train.csv')\n",
    "df_train_slices = pd.read_csv(f'{METADATA_PATH}/train_segmented.csv')\n",
    "df_test = pd.read_csv(f'{RSNA_2022_PATH}/test.csv')\n",
    "\n",
    "#PreProcess and Effnetdata\n",
    "df_train,df_train_slices,df_test,df_test_slices = effnet_data.preprocess(df_train = df_train,df_train_slices=df_train_slices,df_test=df_test,TEST_IMAGES_PATH=TEST_IMAGES_PATH,N_FOLDS=N_FOLDS)\n",
    "ds_train = effnet_data.EffnetDataSet(df_train, TRAIN_IMAGES_PATH, WEIGHTS.transforms())\n",
    "ds_test = effnet_data.EffnetDataSet(df_test, TEST_IMAGES_PATH, WEIGHTS.transforms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"temp\"\n",
    "\n",
    "class EffnetModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        effnet = tv.models.efficientnet_v2_s(weights=WEIGHTS)\n",
    "        self.model = create_feature_extractor(effnet, ['flatten'])\n",
    "        self.nn_fracture = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1280, 7),\n",
    "        )\n",
    "        self.nn_vertebrae = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1280, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # returns logits\n",
    "        x = self.model(x)['flatten']\n",
    "        return self.nn_fracture(x), self.nn_vertebrae(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        frac, vert = self.forward(x)\n",
    "        return torch.sigmoid(frac), torch.sigmoid(vert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_effnet(model: EffnetModel, ds, max_batches=PREDICT_MAX_BATCHES, shuffle=False):\n",
    "    torch.manual_seed(SEED)\n",
    "    model = model.to(DEVICE)\n",
    "    dl_test = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=os.cpu_count(),\n",
    "                                          collate_fn=utils.filter_nones)\n",
    "    pred_frac = []\n",
    "    pred_vert = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        frac_losses = []\n",
    "        vert_losses = []\n",
    "        with tqdm(dl_test, desc='Eval', miniters=1) as progress:\n",
    "            for i, (X, y_frac, y_vert) in enumerate(progress):\n",
    "                with autocast():\n",
    "                    y_frac_pred, y_vert_pred = model.forward(X.to(DEVICE))\n",
    "                    frac_loss = custom_metric.weighted_loss(y_frac_pred, y_frac.to(DEVICE),DEVICE=DEVICE).item()\n",
    "\n",
    "                    #Classification of Bones\n",
    "                    vert_loss = torch.nn.functional.binary_cross_entropy_with_logits(y_vert_pred, y_vert.to(DEVICE)).item()\n",
    "                    pred_frac.append(torch.sigmoid(y_frac_pred))\n",
    "                    pred_vert.append(torch.sigmoid(y_vert_pred))\n",
    "                    frac_losses.append(frac_loss)\n",
    "                    vert_losses.append(vert_loss)\n",
    "                if i >= max_batches:\n",
    "                    break\n",
    "        return np.mean(frac_losses), np.mean(vert_losses), torch.concat(pred_frac).cpu().numpy(), torch.concat(pred_vert).cpu().numpy()\n",
    "\n",
    "def gen_effnet_predictions(effnet_models, df_train):\n",
    "    if os.path.exists(os.path.join(EFFNET_CHECKPOINTS_PATH, 'train_{PROJECT_NAME}_{MODEL_NAME}_predictions.csv')):\n",
    "        print('Found cached version of train_predictions.csv')\n",
    "        df_train_predictions = pd.read_csv(os.path.join(EFFNET_CHECKPOINTS_PATH, 'train_{PROJECT_NAME}_{MODEL_NAME}_predictions.csv'))\n",
    "    else:\n",
    "        df_train_predictions = []\n",
    "        df_eval_predictions = []\n",
    "        with tqdm(enumerate(effnet_models), total=len(effnet_models), desc='Folds') as progress:\n",
    "            for fold, effnet_model in progress:\n",
    "                ds_eval = effnet_data.EffnetDataSet(df_train.query('split == @fold'), TRAIN_IMAGES_PATH, WEIGHTS.transforms())\n",
    "                #ds_train = effnet_data.EffnetDataSet(df_train.query('split != @fold'), TRAIN_IMAGES_PATH, WEIGHTS.transforms())\n",
    "\n",
    "                ##train_prediction\n",
    "                #train_frac_loss, train_vert_loss, train_effnet_pred_frac, train_effnet_pred_vert = evaluate_effnet(effnet_model, ds_train, PREDICT_MAX_BATCHES)\n",
    "                #progress.set_description(f'Fold score:{train_frac_loss:.02f}')\n",
    "                #df_train_effnet_pred = pd.DataFrame(data=np.concatenate([train_effnet_pred_frac, train_effnet_pred_vert], axis=1),\n",
    "                #                              columns=[f'C{i}_effnet_frac' for i in range(1, 8)] +\n",
    "                #                                      [f'C{i}_effnet_vert' for i in range(1, 8)])\n",
    "                #df_train = pd.concat(\n",
    "                #    [df_train.query('split != @fold').head(len(df_train_effnet_pred)).reset_index(drop=True), df_train_effnet_pred],\n",
    "                #    axis=1\n",
    "                #).sort_values(['StudyInstanceUID', 'Slice'])\n",
    "                #df_train_predictions.append(df_train)\n",
    "\n",
    "                #valid_prediction\n",
    "                eval_frac_loss, eval_vert_loss, eval_effnet_pred_frac, eval_effnet_pred_vert = evaluate_effnet(effnet_model, ds_eval, PREDICT_MAX_BATCHES)\n",
    "                progress.set_description(f'Fold score:{eval_frac_loss:.02f}')\n",
    "                df_eval_effnet_pred = pd.DataFrame(data=np.concatenate([eval_effnet_pred_frac, eval_effnet_pred_vert], axis=1),\n",
    "                                              columns=[f'C{i}_effnet_frac' for i in range(1, 8)] +\n",
    "                                                      [f'C{i}_effnet_vert' for i in range(1, 8)])\n",
    "\n",
    "                df_eval = pd.concat(\n",
    "                    [df_train.query('split == @fold').head(len(df_eval_effnet_pred)).reset_index(drop=True), df_eval_effnet_pred],\n",
    "                    axis=1\n",
    "                ).sort_values(['StudyInstanceUID', 'Slice'])\n",
    "                df_eval_predictions.append(df_eval)\n",
    "\n",
    "        #df_train_predictions = pd.concat(df_train_predictions)\n",
    "        df_eval_predictions = pd.concat(df_eval_predictions)\n",
    "\n",
    "        #df_train_predictions.to_csv(f'{EFFNET_CHECKPOINTS_PATH}/{MODEL_NAME}_train_prediction.csv')\n",
    "        df_eval_predictions.to_csv(f'{EFFNET_CHECKPOINTS_PATH}/{MODEL_NAME}_eval_prediction.csv')\n",
    "        #df_train_predictions,\n",
    "    return df_eval_predictions\n",
    "\n",
    "def patient_prediction(df,frac_cols,vert_cols):\n",
    "    c1c7 = np.average(df[frac_cols].values, axis=0, weights=df[vert_cols].values)\n",
    "    pred_patient_overall = 1 - np.prod(1 - c1c7)\n",
    "    return np.concatenate([[pred_patient_overall], c1c7])\n",
    "\n",
    "def evaluate(effnet_models,df_train):\n",
    "    df_eval_pred = gen_effnet_predictions(effnet_models=effnet_models,df_train=df_train)\n",
    "    target_cols = ['patient_overall'] + [f'C{i}_fracture' for i in range(1, 8)]\n",
    "    frac_cols = [f'C{i}_effnet_frac' for i in range(1, 8)]\n",
    "    vert_cols = [f'C{i}_effnet_vert' for i in range(1, 8)]\n",
    "\n",
    "    #df_patient_train_pred = df_train_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df,vert_cols=vert_cols)).to_frame('pred').join(df_train_pred.groupby('StudyInstanceUID')[target_cols].mean())\n",
    "    df_patient_eval_pred = df_eval_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df,frac_cols=frac_cols,vert_cols=vert_cols)).to_frame('pred').join(df_eval_pred.groupby('StudyInstanceUID')[target_cols].mean())\n",
    "\n",
    "    #train_targets = df_patient_train_pred[target_cols].values\n",
    "    #train_predictions = np.stack(df_patient_train_pred.pred.values.tolist())\n",
    "\n",
    "    eval_targets = df_patient_eval_pred[target_cols].values\n",
    "    eval_predictions = np.stack(df_patient_eval_pred.pred.values.tolist())\n",
    "\n",
    "    #print('Train_CV score:', custom_metric.weighted_loss(torch.logit(torch.as_tensor(train_predictions)).to(DEVICE), torch.as_tensor(train_targets).to(DEVICE)))\n",
    "    print('Valid_CV score:', custom_metric.weighted_loss(torch.logit(torch.as_tensor(eval_predictions)).to(DEVICE), torch.as_tensor(eval_targets).to(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet_models = [utils.load_model(EffnetModel(),path = \"/home/jumpei.uchida/develop/kaggle_1080ti_1_2/rsna-2022-cervical-spine-fracture-detection/effnet/src/saved_model/effnet\" ,name = f\"effnet_new_window_fold3-f{i}\") for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   0%|          | 5/7413 [00:05<2:11:53,  1.07s/it]\n",
      "Eval:   0%|          | 5/7413 [00:04<2:01:59,  1.01it/s]8s/it]\n",
      "Eval:   0%|          | 5/7410 [00:05<2:07:40,  1.03s/it]3s/it]\n",
      "Fold score:0.07: 100%|██████████| 3/3 [00:15<00:00,  5.28s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meffnet_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meffnet_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [23], line 79\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(effnet_models, df_train)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(effnet_models,df_train):\n\u001b[0;32m---> 79\u001b[0m     df_train_pred,df_eval_pred \u001b[38;5;241m=\u001b[39m gen_effnet_predictions(effnet_models\u001b[38;5;241m=\u001b[39meffnet_models,df_train\u001b[38;5;241m=\u001b[39mdf_train)\n\u001b[1;32m     80\u001b[0m     target_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_overall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fracture\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)]\n\u001b[1;32m     81\u001b[0m     frac_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_effnet_frac\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "evaluate(effnet_models=effnet_models,df_train=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   0%|          | 5/7413 [00:04<1:58:25,  1.04it/s]\n",
      "Eval:   0%|          | 5/7413 [00:04<2:00:24,  1.03it/s]8s/it]\n",
      "Eval:   0%|          | 5/7410 [00:04<1:45:22,  1.17it/s]2s/it]\n",
      "Fold score:0.07: 100%|██████████| 3/3 [00:14<00:00,  4.73s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_predictions = []\n",
    "df_eval_predictions = []\n",
    "with tqdm(enumerate(effnet_models), total=len(effnet_models), desc='Folds') as progress:\n",
    "    for fold, effnet_model in progress:\n",
    "        ds_eval = effnet_data.EffnetDataSet(df_train.query('split == @fold'), TRAIN_IMAGES_PATH, WEIGHTS.transforms())\n",
    "        #valid_prediction\n",
    "        eval_frac_loss, eval_vert_loss, eval_effnet_pred_frac, eval_effnet_pred_vert = evaluate_effnet(effnet_model, ds_eval, PREDICT_MAX_BATCHES)\n",
    "        progress.set_description(f'Fold score:{eval_frac_loss:.02f}')\n",
    "        df_eval_effnet_pred = pd.DataFrame(data=np.concatenate([eval_effnet_pred_frac, eval_effnet_pred_vert], axis=1),\n",
    "                                        columns=[f'C{i}_effnet_frac' for i in range(1, 8)] +\n",
    "                                                [f'C{i}_effnet_vert' for i in range(1, 8)])\n",
    "\n",
    "        df_eval = pd.concat(\n",
    "            [df_train.query('split == @fold').head(len(df_eval_effnet_pred)).reset_index(drop=True), df_eval_effnet_pred],\n",
    "            axis=1\n",
    "        ).sort_values(['StudyInstanceUID', 'Slice'])\n",
    "        df_eval_predictions.append(df_eval)\n",
    "df_eval_predictions = pd.concat(df_eval_predictions)\n",
    "\n",
    "df_eval_predictions.to_csv(f'{EFFNET_CHECKPOINTS_PATH}/{MODEL_NAME}_eval_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eval_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m vert_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_effnet_vert\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#df_patient_train_pred = df_train_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df,vert_cols=vert_cols)).to_frame('pred').join(df_train_pred.groupby('StudyInstanceUID')[target_cols].mean())\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df_patient_eval_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdf_eval_pred\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudyInstanceUID\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m df: patient_prediction(df,frac_cols\u001b[38;5;241m=\u001b[39mfrac_cols,vert_cols\u001b[38;5;241m=\u001b[39mvert_cols))\u001b[38;5;241m.\u001b[39mto_frame(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(df_eval_pred\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudyInstanceUID\u001b[39m\u001b[38;5;124m'\u001b[39m)[target_cols]\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_eval_pred' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "target_cols = ['patient_overall'] + [f'C{i}_fracture' for i in range(1, 8)]\n",
    "frac_cols = [f'C{i}_effnet_frac' for i in range(1, 8)]\n",
    "vert_cols = [f'C{i}_effnet_vert' for i in range(1, 8)]\n",
    "\n",
    "#df_patient_train_pred = df_train_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df,vert_cols=vert_cols)).to_frame('pred').join(df_train_pred.groupby('StudyInstanceUID')[target_cols].mean())\n",
    "df_patient_eval_pred = df_eval_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df,frac_cols=frac_cols,vert_cols=vert_cols)).to_frame('pred').join(df_eval_pred.groupby('StudyInstanceUID')[target_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rsna-2022-cervical-spine-fracture-detectio--CY1vaQI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9543f5d01a1500c6a5cd37c6db0f31fe3955bfe154c36c5ae281c10861deee7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
